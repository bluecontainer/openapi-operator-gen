// Generated by openapi-operator-gen v0.0.10-13-g20d778f-dirty
// kubectl plugin for petstore operator
// DO NOT EDIT - This file is generated from OpenAPI spec

package cmd

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"sort"
	"strings"

	"github.com/spf13/cobra"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/dynamic"
)

var (
	nodesAllNamespaces bool
	nodesLabelSelector string
)

// rundeckNode represents a single Rundeck resource model node.
type rundeckNode struct {
	NodeName        string `json:"nodename"`
	Hostname        string `json:"hostname"`
	Tags            string `json:"tags"`
	OSFamily        string `json:"osFamily"`
	NodeExecutor    string `json:"node-executor"`
	FileCopier      string `json:"file-copier"`
	TargetType      string `json:"targetType"`
	TargetValue     string `json:"targetValue"`
	TargetNamespace string `json:"targetNamespace"`
	WorkloadKind    string `json:"workloadKind"`
	WorkloadName    string `json:"workloadName"`
	PodCount        string `json:"podCount"`
	HealthyPods     string `json:"healthyPods"`
}

var nodesCmd = &cobra.Command{
	Use:   "nodes",
	Short: "Discover workloads as Rundeck node source JSON",
	Long: `Discover Kubernetes workloads (Helm releases, StatefulSets, Deployments) and
output them as Rundeck resource model JSON. This command is designed to be used
as a Rundeck script-based resource model source.

Each discovered workload becomes a Rundeck node with attributes that map to
the kubectl plugin's --target-* flags:

  targetType:      helm-release, statefulset, or deployment
  targetValue:     the workload or release name
  targetNamespace: the workload's namespace
  workloadKind:    StatefulSet or Deployment
  workloadName:    the underlying workload name
  podCount:        total pod count
  healthyPods:     running pod count

Examples:
  # Discover workloads in the default namespace
  kubectl petstore nodes

  # Discover workloads across all namespaces
  kubectl petstore nodes --all-namespaces

  # Filter by label selector
  kubectl petstore nodes -l app.kubernetes.io/part-of=petstore`,
	RunE: runNodes,
}

func init() {
	nodesCmd.Flags().BoolVarP(&nodesAllNamespaces, "all-namespaces", "A", false, "Discover workloads across all namespaces")
	nodesCmd.Flags().StringVarP(&nodesLabelSelector, "selector", "l", "", "Label selector to filter workloads (e.g., app=myapp)")
}

func runNodes(cmd *cobra.Command, args []string) error {
	ctx := context.Background()
	dynClient := k8sClient.DynamicClient()
	if dynClient == nil {
		return fmt.Errorf("dynamic client not initialized (dry-run mode does not support nodes discovery)")
	}

	ns := k8sClient.GetNamespace()

	stsGVR := schema.GroupVersionResource{Group: "apps", Version: "v1", Resource: "statefulsets"}
	deployGVR := schema.GroupVersionResource{Group: "apps", Version: "v1", Resource: "deployments"}
	podGVR := schema.GroupVersionResource{Group: "", Version: "v1", Resource: "pods"}

	listOpts := metav1.ListOptions{}
	if nodesLabelSelector != "" {
		listOpts.LabelSelector = nodesLabelSelector
	}

	nodes := make(map[string]*rundeckNode)

	// Track Helm releases to deduplicate (multiple workloads per release)
	type helmInfo struct {
		release       string
		namespace     string
		workloadKind  string
		workloadName  string
		totalPods     int
		healthyPods   int
	}
	helmReleases := make(map[string]*helmInfo) // key: "release@namespace"

	// --- Discover StatefulSets ---
	var stsList *unstructured.UnstructuredList
	var err error
	if nodesAllNamespaces {
		stsList, err = dynClient.Resource(stsGVR).Namespace("").List(ctx, listOpts)
	} else {
		stsList, err = dynClient.Resource(stsGVR).Namespace(ns).List(ctx, listOpts)
	}
	if err != nil {
		fmt.Fprintf(os.Stderr, "Warning: failed to list StatefulSets: %v\n", err)
	} else {
		for _, sts := range stsList.Items {
			stsName := sts.GetName()
			stsNS := sts.GetNamespace()
			labels := sts.GetLabels()
			replicas, _, _ := unstructured.NestedInt64(sts.Object, "spec", "replicas")
			if replicas == 0 {
				replicas = 1
			}

			// Count running pods
			healthy := countHealthyPods(ctx, dynClient, podGVR, stsNS, getSelectorLabels(&sts))

			helmRelease := labels["app.kubernetes.io/instance"]
			if helmRelease != "" {
				key := helmRelease + "@" + stsNS
				if existing, ok := helmReleases[key]; ok {
					// Keep the workload with more replicas as the "primary"
					existing.totalPods += int(replicas)
					existing.healthyPods += healthy
					if int(replicas) > 0 && existing.workloadKind == "" {
						existing.workloadKind = "StatefulSet"
						existing.workloadName = stsName
					}
				} else {
					helmReleases[key] = &helmInfo{
						release:      helmRelease,
						namespace:    stsNS,
						workloadKind: "StatefulSet",
						workloadName: stsName,
						totalPods:    int(replicas),
						healthyPods:  healthy,
					}
				}
			} else {
				nodeKey := fmt.Sprintf("sts:%s@%s", stsName, stsNS)
				nodes[nodeKey] = &rundeckNode{
					NodeName:        nodeKey,
					Hostname:        "localhost",
					Tags:            joinTags("statefulset", stsNS),
					OSFamily:        "kubernetes",
					NodeExecutor:    "local",
					FileCopier:      "local",
					TargetType:      "statefulset",
					TargetValue:     stsName,
					TargetNamespace: stsNS,
					WorkloadKind:    "StatefulSet",
					WorkloadName:    stsName,
					PodCount:        fmt.Sprintf("%d", replicas),
					HealthyPods:     fmt.Sprintf("%d", healthy),
				}
			}
		}
	}

	// --- Discover Deployments ---
	var deployList *unstructured.UnstructuredList
	if nodesAllNamespaces {
		deployList, err = dynClient.Resource(deployGVR).Namespace("").List(ctx, listOpts)
	} else {
		deployList, err = dynClient.Resource(deployGVR).Namespace(ns).List(ctx, listOpts)
	}
	if err != nil {
		fmt.Fprintf(os.Stderr, "Warning: failed to list Deployments: %v\n", err)
	} else {
		for _, deploy := range deployList.Items {
			deployName := deploy.GetName()
			deployNS := deploy.GetNamespace()
			labels := deploy.GetLabels()
			replicas, _, _ := unstructured.NestedInt64(deploy.Object, "spec", "replicas")
			if replicas == 0 {
				replicas = 1
			}

			healthy := countHealthyPods(ctx, dynClient, podGVR, deployNS, getSelectorLabels(&deploy))

			helmRelease := labels["app.kubernetes.io/instance"]
			if helmRelease != "" {
				key := helmRelease + "@" + deployNS
				if existing, ok := helmReleases[key]; ok {
					existing.totalPods += int(replicas)
					existing.healthyPods += healthy
				} else {
					helmReleases[key] = &helmInfo{
						release:      helmRelease,
						namespace:    deployNS,
						workloadKind: "Deployment",
						workloadName: deployName,
						totalPods:    int(replicas),
						healthyPods:  healthy,
					}
				}
			} else {
				nodeKey := fmt.Sprintf("deploy:%s@%s", deployName, deployNS)
				nodes[nodeKey] = &rundeckNode{
					NodeName:        nodeKey,
					Hostname:        "localhost",
					Tags:            joinTags("deployment", deployNS),
					OSFamily:        "kubernetes",
					NodeExecutor:    "local",
					FileCopier:      "local",
					TargetType:      "deployment",
					TargetValue:     deployName,
					TargetNamespace: deployNS,
					WorkloadKind:    "Deployment",
					WorkloadName:    deployName,
					PodCount:        fmt.Sprintf("%d", replicas),
					HealthyPods:     fmt.Sprintf("%d", healthy),
				}
			}
		}
	}

	// --- Add Helm release nodes ---
	for _, info := range helmReleases {
		nodeKey := fmt.Sprintf("helm:%s@%s", info.release, info.namespace)
		nodes[nodeKey] = &rundeckNode{
			NodeName:        nodeKey,
			Hostname:        "localhost",
			Tags:            joinTags("helm-release", info.namespace),
			OSFamily:        "kubernetes",
			NodeExecutor:    "local",
			FileCopier:      "local",
			TargetType:      "helm-release",
			TargetValue:     info.release,
			TargetNamespace: info.namespace,
			WorkloadKind:    info.workloadKind,
			WorkloadName:    info.workloadName,
			PodCount:        fmt.Sprintf("%d", info.totalPods),
			HealthyPods:     fmt.Sprintf("%d", info.healthyPods),
		}
	}

	// Output as Rundeck resource JSON (sorted by key for deterministic output)
	keys := make([]string, 0, len(nodes))
	for k := range nodes {
		keys = append(keys, k)
	}
	sort.Strings(keys)

	output := make(map[string]*rundeckNode, len(nodes))
	for _, k := range keys {
		output[k] = nodes[k]
	}

	enc := json.NewEncoder(os.Stdout)
	enc.SetIndent("", "  ")
	return enc.Encode(output)
}

// getSelectorLabels extracts spec.selector.matchLabels from a workload.
func getSelectorLabels(obj *unstructured.Unstructured) map[string]string {
	labels, found, err := unstructured.NestedStringMap(obj.Object, "spec", "selector", "matchLabels")
	if err != nil || !found {
		return nil
	}
	return labels
}

// countHealthyPods counts running pods matching the given selector labels.
func countHealthyPods(ctx context.Context, dynClient dynamic.Interface, podGVR schema.GroupVersionResource, namespace string, selectorLabels map[string]string) int {
	if len(selectorLabels) == 0 {
		return 0
	}

	parts := make([]string, 0, len(selectorLabels))
	for k, v := range selectorLabels {
		parts = append(parts, k+"="+v)
	}
	selector := strings.Join(parts, ",")

	pods, err := dynClient.Resource(podGVR).Namespace(namespace).List(ctx, metav1.ListOptions{
		LabelSelector: selector,
	})
	if err != nil {
		return 0
	}

	healthy := 0
	for _, pod := range pods.Items {
		phase, _, _ := unstructured.NestedString(pod.Object, "status", "phase")
		if phase == "Running" {
			healthy++
		}
	}
	return healthy
}

// joinTags creates a comma-separated tag string for Rundeck nodes.
func joinTags(workloadType, namespace string) string {
	return workloadType + "," + namespace
}
