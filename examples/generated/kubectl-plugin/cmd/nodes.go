// Generated by openapi-operator-gen v0.0.10-18-g2510b7b-dirty
// kubectl plugin for petstore operator
// DO NOT EDIT - This file is generated from OpenAPI spec

package cmd

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"sort"
	"strings"

	"github.com/spf13/cobra"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/dynamic"
)

const (
	// defaultClusterTokenSuffix is the default Rundeck Key Storage path suffix for the cluster token.
	// Used when --cluster-token-suffix is not specified. Jobs prepend "keys/" to this value.
	defaultClusterTokenSuffix = "project/petstore-operator/k8s-token"
)

var (
	nodesAllNamespaces     bool
	nodesLabelSelector     string
	nodesClusterName       string
	nodesClusterURL        string
	nodesClusterTokenSuffix string
)

// rundeckNode represents a single Rundeck resource model node.
type rundeckNode struct {
	NodeName         string `json:"nodename"`
	Hostname         string `json:"hostname"`
	Tags             string `json:"tags"`
	OSFamily         string `json:"osFamily"`
	NodeExecutor     string `json:"node-executor"`
	FileCopier       string `json:"file-copier"`
	Cluster            string `json:"cluster,omitempty"`
	ClusterURL         string `json:"clusterUrl,omitempty"`
	ClusterTokenSuffix string `json:"clusterTokenSuffix,omitempty"`
	TargetType       string `json:"targetType"`
	TargetValue      string `json:"targetValue"`
	TargetNamespace  string `json:"targetNamespace"`
	WorkloadKind     string `json:"workloadKind"`
	WorkloadName     string `json:"workloadName"`
	PodCount         string `json:"podCount"`
	HealthyPods      string `json:"healthyPods"`
}

var nodesCmd = &cobra.Command{
	Use:   "nodes",
	Short: "Discover workloads as Rundeck node source JSON",
	Long: `Discover Kubernetes workloads (Helm releases, StatefulSets, Deployments) and
output them as Rundeck resource model JSON. This command is designed to be used
as a Rundeck script-based resource model source.

Each discovered workload becomes a Rundeck node with attributes that map to
the kubectl plugin's --target-* flags:

  targetType:      helm-release, statefulset, or deployment
  targetValue:     the workload or release name
  targetNamespace: the workload's namespace
  workloadKind:    StatefulSet or Deployment
  workloadName:    the underlying workload name
  podCount:        total pod count
  healthyPods:     running pod count

Examples:
  # Discover workloads in the default namespace
  kubectl petstore nodes

  # Discover workloads across all namespaces
  kubectl petstore nodes --all-namespaces

  # Filter by label selector
  kubectl petstore nodes -l app.kubernetes.io/part-of=petstore`,
	RunE: runNodes,
}

func init() {
	nodesCmd.Flags().BoolVarP(&nodesAllNamespaces, "all-namespaces", "A", false, "Discover workloads across all namespaces")
	nodesCmd.Flags().StringVarP(&nodesLabelSelector, "selector", "l", "", "Label selector to filter workloads (e.g., app=myapp)")
	nodesCmd.Flags().StringVar(&nodesClusterName, "cluster-name", "", "Cluster identifier for multi-cluster node discovery")
	nodesCmd.Flags().StringVar(&nodesClusterURL, "cluster-url", "", "Cluster API URL to embed in node attributes")
	nodesCmd.Flags().StringVar(&nodesClusterTokenSuffix, "cluster-token-suffix", "", "Rundeck Key Storage path suffix for cluster token (e.g., clusters/prod/token). Jobs prepend 'keys/' to this value.")
}

func runNodes(cmd *cobra.Command, args []string) error {
	ctx := context.Background()
	dynClient := k8sClient.DynamicClient()
	if dynClient == nil {
		return fmt.Errorf("dynamic client not initialized (dry-run mode does not support nodes discovery)")
	}

	ns := k8sClient.GetNamespace()

	stsGVR := schema.GroupVersionResource{Group: "apps", Version: "v1", Resource: "statefulsets"}
	deployGVR := schema.GroupVersionResource{Group: "apps", Version: "v1", Resource: "deployments"}
	podGVR := schema.GroupVersionResource{Group: "", Version: "v1", Resource: "pods"}

	listOpts := metav1.ListOptions{}
	if nodesLabelSelector != "" {
		listOpts.LabelSelector = nodesLabelSelector
	}

	nodes := make(map[string]*rundeckNode)

	// Track Helm releases to deduplicate (multiple workloads per release)
	type helmInfo struct {
		release       string
		namespace     string
		workloadKind  string
		workloadName  string
		totalPods     int
		healthyPods   int
	}
	helmReleases := make(map[string]*helmInfo) // key: "release@namespace"

	// --- Discover StatefulSets ---
	var stsList *unstructured.UnstructuredList
	var err error
	if nodesAllNamespaces {
		stsList, err = dynClient.Resource(stsGVR).Namespace("").List(ctx, listOpts)
	} else {
		stsList, err = dynClient.Resource(stsGVR).Namespace(ns).List(ctx, listOpts)
	}
	if err != nil {
		fmt.Fprintf(os.Stderr, "Warning: failed to list StatefulSets: %v\n", err)
	} else {
		for _, sts := range stsList.Items {
			stsName := sts.GetName()
			stsNS := sts.GetNamespace()
			labels := sts.GetLabels()
			replicas, _, _ := unstructured.NestedInt64(sts.Object, "spec", "replicas")
			if replicas == 0 {
				replicas = 1
			}

			// Count running pods
			healthy := countHealthyPods(ctx, dynClient, podGVR, stsNS, getSelectorLabels(&sts))

			helmRelease := labels["app.kubernetes.io/instance"]
			if helmRelease != "" {
				key := helmRelease + "@" + stsNS
				if existing, ok := helmReleases[key]; ok {
					// Keep the workload with more replicas as the "primary"
					existing.totalPods += int(replicas)
					existing.healthyPods += healthy
					if int(replicas) > 0 && existing.workloadKind == "" {
						existing.workloadKind = "StatefulSet"
						existing.workloadName = stsName
					}
				} else {
					helmReleases[key] = &helmInfo{
						release:      helmRelease,
						namespace:    stsNS,
						workloadKind: "StatefulSet",
						workloadName: stsName,
						totalPods:    int(replicas),
						healthyPods:  healthy,
					}
				}
			} else {
				nodeKey := makeNodeKey("sts", stsName, stsNS)
				nodes[nodeKey] = &rundeckNode{
					NodeName:         nodeKey,
					Hostname:         "localhost",
					Tags:             joinTags("statefulset", stsNS, nodesClusterName),
					OSFamily:         "kubernetes",
					NodeExecutor:     "local",
					FileCopier:       "local",
					Cluster:          nodesClusterName,
					ClusterURL:       nodesClusterURL,
					ClusterTokenSuffix: getClusterTokenSuffix(),
					TargetType:       "statefulset",
					TargetValue:      stsName,
					TargetNamespace:  stsNS,
					WorkloadKind:     "StatefulSet",
					WorkloadName:     stsName,
					PodCount:         fmt.Sprintf("%d", replicas),
					HealthyPods:      fmt.Sprintf("%d", healthy),
				}
			}
		}
	}

	// --- Discover Deployments ---
	var deployList *unstructured.UnstructuredList
	if nodesAllNamespaces {
		deployList, err = dynClient.Resource(deployGVR).Namespace("").List(ctx, listOpts)
	} else {
		deployList, err = dynClient.Resource(deployGVR).Namespace(ns).List(ctx, listOpts)
	}
	if err != nil {
		fmt.Fprintf(os.Stderr, "Warning: failed to list Deployments: %v\n", err)
	} else {
		for _, deploy := range deployList.Items {
			deployName := deploy.GetName()
			deployNS := deploy.GetNamespace()
			labels := deploy.GetLabels()
			replicas, _, _ := unstructured.NestedInt64(deploy.Object, "spec", "replicas")
			if replicas == 0 {
				replicas = 1
			}

			healthy := countHealthyPods(ctx, dynClient, podGVR, deployNS, getSelectorLabels(&deploy))

			helmRelease := labels["app.kubernetes.io/instance"]
			if helmRelease != "" {
				key := helmRelease + "@" + deployNS
				if existing, ok := helmReleases[key]; ok {
					existing.totalPods += int(replicas)
					existing.healthyPods += healthy
				} else {
					helmReleases[key] = &helmInfo{
						release:      helmRelease,
						namespace:    deployNS,
						workloadKind: "Deployment",
						workloadName: deployName,
						totalPods:    int(replicas),
						healthyPods:  healthy,
					}
				}
			} else {
				nodeKey := makeNodeKey("deploy", deployName, deployNS)
				nodes[nodeKey] = &rundeckNode{
					NodeName:         nodeKey,
					Hostname:         "localhost",
					Tags:             joinTags("deployment", deployNS, nodesClusterName),
					OSFamily:         "kubernetes",
					NodeExecutor:     "local",
					FileCopier:       "local",
					Cluster:          nodesClusterName,
					ClusterURL:       nodesClusterURL,
					ClusterTokenSuffix: getClusterTokenSuffix(),
					TargetType:       "deployment",
					TargetValue:      deployName,
					TargetNamespace:  deployNS,
					WorkloadKind:     "Deployment",
					WorkloadName:     deployName,
					PodCount:         fmt.Sprintf("%d", replicas),
					HealthyPods:      fmt.Sprintf("%d", healthy),
				}
			}
		}
	}

	// --- Add Helm release nodes ---
	for _, info := range helmReleases {
		nodeKey := makeNodeKey("helm", info.release, info.namespace)
		nodes[nodeKey] = &rundeckNode{
			NodeName:         nodeKey,
			Hostname:         "localhost",
			Tags:             joinTags("helm-release", info.namespace, nodesClusterName),
			OSFamily:         "kubernetes",
			NodeExecutor:     "local",
			FileCopier:       "local",
			Cluster:          nodesClusterName,
			ClusterURL:       nodesClusterURL,
			ClusterTokenSuffix: getClusterTokenSuffix(),
			TargetType:       "helm-release",
			TargetValue:      info.release,
			TargetNamespace:  info.namespace,
			WorkloadKind:     info.workloadKind,
			WorkloadName:     info.workloadName,
			PodCount:         fmt.Sprintf("%d", info.totalPods),
			HealthyPods:      fmt.Sprintf("%d", info.healthyPods),
		}
	}

	// Output as Rundeck resource JSON (sorted by key for deterministic output)
	keys := make([]string, 0, len(nodes))
	for k := range nodes {
		keys = append(keys, k)
	}
	sort.Strings(keys)

	output := make(map[string]*rundeckNode, len(nodes))
	for _, k := range keys {
		output[k] = nodes[k]
	}

	enc := json.NewEncoder(os.Stdout)
	enc.SetIndent("", "  ")
	return enc.Encode(output)
}

// getSelectorLabels extracts spec.selector.matchLabels from a workload.
func getSelectorLabels(obj *unstructured.Unstructured) map[string]string {
	labels, found, err := unstructured.NestedStringMap(obj.Object, "spec", "selector", "matchLabels")
	if err != nil || !found {
		return nil
	}
	return labels
}

// countHealthyPods counts running pods matching the given selector labels.
func countHealthyPods(ctx context.Context, dynClient dynamic.Interface, podGVR schema.GroupVersionResource, namespace string, selectorLabels map[string]string) int {
	if len(selectorLabels) == 0 {
		return 0
	}

	parts := make([]string, 0, len(selectorLabels))
	for k, v := range selectorLabels {
		parts = append(parts, k+"="+v)
	}
	selector := strings.Join(parts, ",")

	pods, err := dynClient.Resource(podGVR).Namespace(namespace).List(ctx, metav1.ListOptions{
		LabelSelector: selector,
	})
	if err != nil {
		return 0
	}

	healthy := 0
	for _, pod := range pods.Items {
		phase, _, _ := unstructured.NestedString(pod.Object, "status", "phase")
		if phase == "Running" {
			healthy++
		}
	}
	return healthy
}

// joinTags creates a comma-separated tag string for Rundeck nodes.
func joinTags(workloadType, namespace, cluster string) string {
	if cluster != "" {
		return workloadType + "," + namespace + "," + cluster
	}
	return workloadType + "," + namespace
}

// makeNodeKey creates a unique node key, optionally prefixed with cluster name.
func makeNodeKey(workloadType, name, namespace string) string {
	baseKey := fmt.Sprintf("%s:%s@%s", workloadType, name, namespace)
	if nodesClusterName != "" {
		return nodesClusterName + "/" + baseKey
	}
	return baseKey
}

// getClusterTokenSuffix returns the effective token path suffix, using the default if not specified.
// This suffix is used with "keys/${node.clusterTokenSuffix}" in Rundeck job storagePath.
func getClusterTokenSuffix() string {
	if nodesClusterTokenSuffix != "" {
		return nodesClusterTokenSuffix
	}
	return defaultClusterTokenSuffix
}
