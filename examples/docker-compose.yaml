# Docker Compose for Petstore API + Operator Development
#
# This compose file runs the Swagger Petstore API and optionally the operator
# for local development and testing.
#
# Usage:
#   # Start only the petstore API
#   docker compose up -d petstore
#
#   # Build the operator image
#   docker compose build operator
#
#   # Run with k3s (lightweight Kubernetes cluster in Docker)
#   docker compose --profile k3s up -d
#
#   # Access k3s from host (kubeconfig is copied to ./k3s-output/)
#   KUBECONFIG=./k3s-output/kubeconfig.yaml kubectl get nodes
#
#   # Run operator with local kubeconfig (kind/minikube)
#   docker compose --profile with-k8s up -d
#
#   # Run with two petstore instances (multi-endpoint fan-out testing)
#   docker compose --profile multi-endpoint up -d
#
#   # Deploy petstore + operator inside k3s as Kubernetes workloads
#   docker compose --profile k3s-deploy up -d
#   KUBECONFIG=./k3s-deploy-output/kubeconfig.yaml kubectl get pods -n petstore-system
#
#   # For local development without Docker:
#   cd examples/generated
#   go run ./cmd/manager --base-url=http://localhost:8080/api/v3
#
# Note: The 'k3s' profile provides a lightweight Kubernetes cluster in Docker.
# For connecting to an existing cluster, use 'with-k8s' with kind or minikube.
# The 'multi-endpoint' profile runs two petstore instances for testing baseURLs fan-out.
# The 'k3s-deploy' profile deploys both petstore and the operator inside k3s as Kubernetes workloads.

services:
  petstore:
    image: swaggerapi/petstore3:unstable
    profiles:
      - k3s
      - with-k8s
    ports:
      - "8080:8080"
    environment:
      - SWAGGER_HOST=http://localhost:8080
      - SWAGGER_URL=http://localhost:8080
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/api/v3/openapi.json"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Lightweight Kubernetes cluster using k3s
  # Provides a full Kubernetes API without the overhead of kind/minikube
  k3s:
    image: rancher/k3s:v1.29.0-k3s1
    command: server --disable traefik --disable servicelb --disable metrics-server --tls-san k3s
    privileged: true
    profiles:
      - k3s
    ports:
      - "6443:6443"
    environment:
      - K3S_KUBECONFIG_OUTPUT=/output/kubeconfig.yaml
      - K3S_KUBECONFIG_MODE=666
    volumes:
      - k3s-data:/var/lib/rancher/k3s
      - k3s-kubeconfig:/output
    healthcheck:
      test: ["CMD", "kubectl", "get", "nodes"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s

  # Fix kubeconfig and apply CRDs to k3s cluster
  k3s-init:
    image: bitnami/kubectl:latest
    user: root
    profiles:
      - k3s
    volumes:
      - k3s-kubeconfig:/kubeconfig
      - ./generated/config/crd/bases:/crds:ro
      - ./k3s-output:/host-output
    environment:
      - KUBECONFIG=/kubeconfig/kubeconfig-fixed.yaml
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Waiting for kubeconfig..."
        until [ -f /kubeconfig/kubeconfig.yaml ]; do sleep 1; done
        echo "Fixing kubeconfig server address for container access..."
        sed 's/127.0.0.1/k3s/g' /kubeconfig/kubeconfig.yaml > /kubeconfig/kubeconfig-fixed.yaml
        chmod 644 /kubeconfig/kubeconfig-fixed.yaml
        echo "Copying kubeconfig for host access..."
        cp /kubeconfig/kubeconfig.yaml /host-output/kubeconfig.yaml
        chmod 644 /host-output/kubeconfig.yaml
        echo "Waiting for k3s API..."
        until kubectl get nodes; do sleep 2; done
        echo "Applying CRDs..."
        kubectl apply -f /crds/
        echo "CRDs applied successfully"
        sleep infinity
    depends_on:
      k3s:
        condition: service_healthy

  # The operator container - connects to k3s
  operator-k3s:
    build:
      context: ..
      dockerfile: examples/Dockerfile.operator
    image: petstore-operator:local
    profiles:
      - k3s
    environment:
      - REST_API_BASE_URL=http://petstore:8080/api/v3
      - KUBECONFIG=/kubeconfig/kubeconfig-fixed.yaml
      - OPERATOR_LOG_LEVEL=debug
    volumes:
      - k3s-kubeconfig:/kubeconfig:ro
    depends_on:
      petstore:
        condition: service_healthy
      k3s-init:
        condition: service_started

  # The operator container - connects to local Kubernetes cluster
  operator:
    build:
      context: ..
      dockerfile: examples/Dockerfile.operator
    image: petstore-operator:local
    profiles:
      - with-k8s
    environment:
      - REST_API_BASE_URL=http://petstore:8080/api/v3
      - KUBECONFIG=/root/.kube/config
    volumes:
      # Mount kubeconfig for local Kubernetes access (kind/minikube)
      - ${HOME}/.kube:/root/.kube:ro
    depends_on:
      petstore:
        condition: service_healthy
    # For kind clusters, the API server is on the host network
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # ============================================================================
  # Multi-endpoint profile: Two petstore instances for testing baseURLs fan-out
  # ============================================================================

  # First petstore instance (port 8081)
  petstore-1:
    image: swaggerapi/petstore3:unstable
    profiles:
      - multi-endpoint
    ports:
      - "8081:8080"
    environment:
      - SWAGGER_HOST=http://localhost:8081
      - SWAGGER_URL=http://localhost:8081
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/api/v3/openapi.json"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Second petstore instance (port 8082)
  petstore-2:
    image: swaggerapi/petstore3:unstable
    profiles:
      - multi-endpoint
    ports:
      - "8082:8080"
    environment:
      - SWAGGER_HOST=http://localhost:8082
      - SWAGGER_URL=http://localhost:8082
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/api/v3/openapi.json"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # k3s for multi-endpoint testing
  k3s-multi:
    image: rancher/k3s:v1.29.0-k3s1
    command: server --disable traefik --disable servicelb --disable metrics-server --tls-san k3s-multi
    privileged: true
    profiles:
      - multi-endpoint
    ports:
      - "6444:6443"
    environment:
      - K3S_KUBECONFIG_OUTPUT=/output/kubeconfig.yaml
      - K3S_KUBECONFIG_MODE=666
    volumes:
      - k3s-multi-data:/var/lib/rancher/k3s
      - k3s-multi-kubeconfig:/output
    healthcheck:
      test: ["CMD", "kubectl", "get", "nodes"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s

  # Initialize k3s for multi-endpoint
  k3s-multi-init:
    image: bitnami/kubectl:latest
    user: root
    profiles:
      - multi-endpoint
    volumes:
      - k3s-multi-kubeconfig:/kubeconfig
      - ./generated/config/crd/bases:/crds:ro
      - ./k3s-multi-output:/host-output
    environment:
      - KUBECONFIG=/kubeconfig/kubeconfig-fixed.yaml
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Waiting for kubeconfig..."
        until [ -f /kubeconfig/kubeconfig.yaml ]; do sleep 1; done
        echo "Fixing kubeconfig server address for container access..."
        sed 's/127.0.0.1/k3s-multi/g' /kubeconfig/kubeconfig.yaml > /kubeconfig/kubeconfig-fixed.yaml
        chmod 644 /kubeconfig/kubeconfig-fixed.yaml
        echo "Copying kubeconfig for host access (use port 6444)..."
        sed 's/6443/6444/g' /kubeconfig/kubeconfig.yaml > /host-output/kubeconfig.yaml
        chmod 644 /host-output/kubeconfig.yaml
        echo "Waiting for k3s API..."
        until kubectl get nodes; do sleep 2; done
        echo "Applying CRDs..."
        kubectl apply -f /crds/
        echo "CRDs applied successfully"
        sleep infinity
    depends_on:
      k3s-multi:
        condition: service_healthy

  # Operator configured with two base URLs for fan-out testing
  # Uses REST_API_BASE_URLS to configure global fan-out to both petstore instances.
  # Writes will be sent to ALL URLs, reads use first successful response.
  operator-multi:
    build:
      context: ..
      dockerfile: examples/Dockerfile.operator
    image: petstore-operator:local
    profiles:
      - multi-endpoint
    environment:
      # Global fan-out to both petstore instances (comma-separated)
      - REST_API_BASE_URLS=http://petstore-1:8080/api/v3,http://petstore-2:8080/api/v3
      - KUBECONFIG=/kubeconfig/kubeconfig-fixed.yaml
      - OPERATOR_LOG_LEVEL=debug
    volumes:
      - k3s-multi-kubeconfig:/kubeconfig:ro
    depends_on:
      petstore-1:
        condition: service_healthy
      petstore-2:
        condition: service_healthy
      k3s-multi-init:
        condition: service_started

  # ============================================================================
  # k3s-deploy profile: Deploy petstore + operator INSIDE k3s as K8s workloads
  # ============================================================================
  # Unlike the 'k3s' profile where petstore and the operator run as Docker
  # containers alongside k3s, this profile deploys both as Kubernetes
  # Deployments inside the k3s cluster.

  # k3s cluster for in-cluster deployment
  k3s-deploy:
    image: rancher/k3s:v1.29.0-k3s1
    command: server --disable traefik --disable servicelb --disable metrics-server --tls-san k3s-deploy
    privileged: true
    profiles:
      - k3s-deploy
    ports:
      - "6445:6443"
    environment:
      - K3S_KUBECONFIG_OUTPUT=/output/kubeconfig.yaml
      - K3S_KUBECONFIG_MODE=666
    volumes:
      - k3s-deploy-data:/var/lib/rancher/k3s
      - k3s-deploy-kubeconfig:/output
      - k3s-deploy-images:/images
      - k3s-deploy-run:/run/k3s
    healthcheck:
      test: ["CMD", "kubectl", "get", "nodes"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s

  # Build operator image and save as tarball for k3s import
  k3s-deploy-image-save:
    image: docker:cli
    profiles:
      - k3s-deploy
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - k3s-deploy-images:/images
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Saving petstore-operator:local image as tarball..."
        until docker image inspect petstore-operator:local >/dev/null 2>&1; do
          echo "Waiting for petstore-operator:local image to be built..."
          sleep 2
        done
        docker save petstore-operator:local -o /images/petstore-operator.tar
        echo "Saving swaggerapi/petstore3:unstable image as tarball..."
        docker pull swaggerapi/petstore3:unstable
        docker save swaggerapi/petstore3:unstable -o /images/petstore3.tar
        echo "Downloading helm binary..."
        wget -q https://get.helm.sh/helm-v3.14.0-linux-amd64.tar.gz -O /tmp/helm.tar.gz
        tar -xzf /tmp/helm.tar.gz -C /tmp
        cp /tmp/linux-amd64/helm /images/helm
        chmod +x /images/helm
        echo "Images and tools saved successfully"
    depends_on:
      k3s-deploy-operator-build:
        condition: service_completed_successfully

  # Build the operator image (same as other profiles)
  # Runs --version to exit immediately after build, since the operator
  # image is distroless and has no shell.
  k3s-deploy-operator-build:
    build:
      context: ..
      dockerfile: examples/Dockerfile.operator
    image: petstore-operator:local
    profiles:
      - k3s-deploy
    entrypoint: ["/manager", "--version"]

  # Import images into k3s and deploy all workloads
  k3s-deploy-init:
    image: rancher/k3s:v1.29.0-k3s1
    profiles:
      - k3s-deploy
    network_mode: "service:k3s-deploy"
    volumes:
      - k3s-deploy-kubeconfig:/kubeconfig
      - k3s-deploy-images:/images:ro
      - k3s-deploy-run:/run/k3s:ro
      - ./generated/chart/petstore:/chart/petstore:ro
      - ./k3s-deploy/petstore.yaml:/manifests/petstore.yaml:ro
      - ./k3s-deploy-output:/host-output
    environment:
      - KUBECONFIG=/kubeconfig/kubeconfig-fixed.yaml
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "=== k3s-deploy-init: Setting up in-cluster deployment ==="

        echo "Waiting for kubeconfig..."
        until [ -f /kubeconfig/kubeconfig.yaml ]; do sleep 1; done

        echo "Fixing kubeconfig for container access..."
        # Use localhost since we share network namespace with k3s-deploy
        cp /kubeconfig/kubeconfig.yaml /kubeconfig/kubeconfig-fixed.yaml
        chmod 644 /kubeconfig/kubeconfig-fixed.yaml

        echo "Copying kubeconfig for host access (port 6445)..."
        sed 's/6443/6445/g' /kubeconfig/kubeconfig.yaml > /host-output/kubeconfig.yaml
        chmod 644 /host-output/kubeconfig.yaml

        echo "Waiting for k3s API..."
        until kubectl get nodes; do sleep 2; done

        echo "--- Importing container images into k3s ---"
        until [ -f /images/petstore-operator.tar ] && [ -f /images/petstore3.tar ]; do
          echo "Waiting for image tarballs..."
          sleep 2
        done
        echo "Waiting for containerd socket..."
        until [ -S /run/k3s/containerd/containerd.sock ]; do
          echo "  socket not ready yet..."
          sleep 2
        done
        echo "Importing petstore-operator image..."
        ctr --address /run/k3s/containerd/containerd.sock --namespace k8s.io images import /images/petstore-operator.tar
        echo "Importing petstore3 image..."
        ctr --address /run/k3s/containerd/containerd.sock --namespace k8s.io images import /images/petstore3.tar
        echo "Images imported successfully"

        echo "--- Installing Helm ---"
        cp /images/helm /bin/helm
        helm version

        echo "--- Creating namespace ---"
        kubectl create namespace petstore-system

        echo "--- Deploying petstore API ---"
        kubectl apply -f /manifests/petstore.yaml

        echo "--- Installing operator via Helm chart ---"
        helm install petstore /chart/petstore \
          -n petstore-system \
          --set controllerManager.manager.image.repository=docker.io/library/petstore-operator \
          --set controllerManager.manager.image.tag=local \
          --set controllerManager.manager.imagePullPolicy=Never \
          --set 'controllerManager.manager.args={--leader-elect,--namespace-scoped,--deployment-name=petstore,--base-path=/api/v3}'

        echo "--- Waiting for petstore to be ready ---"
        kubectl rollout status deployment/petstore -n petstore-system --timeout=120s

        echo "--- Waiting for operator to be ready ---"
        kubectl rollout status deployment/petstore-controller-manager -n petstore-system --timeout=120s

        echo "=== All workloads deployed successfully ==="
        echo "Access the cluster:"
        echo "  KUBECONFIG=./k3s-deploy-output/kubeconfig.yaml kubectl get pods -n petstore-system"
    depends_on:
      k3s-deploy:
        condition: service_healthy
      k3s-deploy-image-save:
        condition: service_completed_successfully

volumes:
  k3s-data:
  k3s-kubeconfig:
  k3s-multi-data:
  k3s-multi-kubeconfig:
  k3s-deploy-data:
  k3s-deploy-kubeconfig:
  k3s-deploy-images:
  k3s-deploy-run:
