# Generated by openapi-operator-gen {{ .GeneratorVersion }}
# Docker Compose for {{ .AppName }} Operator Development
#
# Usage:
#   # Build the operator image
#   docker compose build operator
#
#   # Run with k3s (lightweight Kubernetes cluster in Docker)
#   docker compose --profile k3s up -d
#
#   # Access k3s from host (kubeconfig is copied to ./k3s-output/)
#   KUBECONFIG=./k3s-output/kubeconfig.yaml kubectl get nodes
#
#   # Run operator with local kubeconfig (kind/minikube)
#   docker compose --profile with-k8s up -d
#
#   # Deploy operator inside k3s as Kubernetes workloads
#   docker compose --profile k3s-deploy up -d
#   KUBECONFIG=./k3s-deploy-output/kubeconfig.yaml kubectl get pods -n {{ .Namespace }}
{{- if .HasRundeckProject }}
#
#   # Rundeck is included in k3s-deploy profile (port 4440)
#   open http://localhost:4440   # Login: admin / admin
{{- end }}

services:
{{- if .HasTargetAPI }}
  {{ .AppName }}:
    image: {{ .TargetAPIImage }}
    profiles:
      - k3s
      - with-k8s
    ports:
      - "{{ .ContainerPort }}:{{ .ContainerPort }}"
{{- if .BasePath }}
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:{{ .ContainerPort }}{{ .BasePath }}/openapi.json"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
{{- end }}
{{- end }}

  # Lightweight Kubernetes cluster using k3s
  k3s:
    image: rancher/k3s:v1.29.0-k3s1
    command: server --disable traefik --disable servicelb --disable metrics-server --tls-san k3s
    privileged: true
    profiles:
      - k3s
    ports:
      - "6443:6443"
    environment:
      - K3S_KUBECONFIG_OUTPUT=/output/kubeconfig.yaml
      - K3S_KUBECONFIG_MODE=666
    volumes:
      - k3s-data:/var/lib/rancher/k3s
      - k3s-kubeconfig:/output
    healthcheck:
      test: ["CMD", "kubectl", "get", "nodes"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s

  # Fix kubeconfig and apply CRDs to k3s cluster
  k3s-init:
    image: bitnami/kubectl:latest
    user: root
    profiles:
      - k3s
    volumes:
      - k3s-kubeconfig:/kubeconfig
      - ./config/crd/bases:/crds:ro
      - ./k3s-output:/host-output
    environment:
      - KUBECONFIG=/kubeconfig/kubeconfig-fixed.yaml
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Waiting for kubeconfig..."
        until [ -f /kubeconfig/kubeconfig.yaml ]; do sleep 1; done
        echo "Fixing kubeconfig server address for container access..."
        sed 's/127.0.0.1/k3s/g' /kubeconfig/kubeconfig.yaml > /kubeconfig/kubeconfig-fixed.yaml
        chmod 644 /kubeconfig/kubeconfig-fixed.yaml
        echo "Copying kubeconfig for host access..."
        cp /kubeconfig/kubeconfig.yaml /host-output/kubeconfig.yaml
        chmod 644 /host-output/kubeconfig.yaml
        echo "Waiting for k3s API..."
        until kubectl get nodes; do sleep 2; done
        echo "Applying CRDs..."
        kubectl apply -f /crds/
        echo "CRDs applied successfully"
        sleep infinity
    depends_on:
      k3s:
        condition: service_healthy

  # The operator container - connects to k3s
  operator-k3s:
    build:
      context: .
      dockerfile: Dockerfile
    image: {{ .AppName }}-operator:local
    profiles:
      - k3s
    environment:
      - KUBECONFIG=/kubeconfig/kubeconfig-fixed.yaml
      - OPERATOR_LOG_LEVEL=debug
{{- if .HasTargetAPI }}
      - REST_API_BASE_URL=http://{{ .AppName }}:{{ .ContainerPort }}{{ .BasePath }}
{{- end }}
    volumes:
      - k3s-kubeconfig:/kubeconfig:ro
    depends_on:
{{- if .HasTargetAPI }}
      {{ .AppName }}:
        condition: service_{{ if .BasePath }}healthy{{ else }}started{{ end }}
{{- end }}
      k3s-init:
        condition: service_started

  # The operator container - connects to local Kubernetes cluster
  operator:
    build:
      context: .
      dockerfile: Dockerfile
    image: {{ .AppName }}-operator:local
    profiles:
      - with-k8s
    environment:
      - KUBECONFIG=/root/.kube/config
{{- if .HasTargetAPI }}
      - REST_API_BASE_URL=http://{{ .AppName }}:{{ .ContainerPort }}{{ .BasePath }}
{{- end }}
    volumes:
      - ${HOME}/.kube:/root/.kube:ro
{{- if .HasTargetAPI }}
    depends_on:
      {{ .AppName }}:
        condition: service_{{ if .BasePath }}healthy{{ else }}started{{ end }}
{{- end }}
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # ============================================================================
  # k3s-deploy profile: Deploy operator INSIDE k3s as Kubernetes workloads
  # ============================================================================

  # k3s cluster for in-cluster deployment
  k3s-deploy:
    image: rancher/k3s:v1.29.0-k3s1
    command: server --disable traefik --disable servicelb --disable metrics-server --tls-san k3s-deploy
    privileged: true
    profiles:
      - k3s-deploy
    ports:
      - "6445:6443"
    environment:
      - K3S_KUBECONFIG_OUTPUT=/output/kubeconfig.yaml
      - K3S_KUBECONFIG_MODE=666
    volumes:
      - k3s-deploy-data:/var/lib/rancher/k3s
      - k3s-deploy-kubeconfig:/output
      - k3s-deploy-images:/images
      - k3s-deploy-run:/run/k3s
    healthcheck:
      test: ["CMD", "kubectl", "get", "nodes"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 30s

  # Build operator image and save as tarball for k3s import
  k3s-deploy-image-save:
    image: docker:cli
    profiles:
      - k3s-deploy
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - k3s-deploy-images:/images
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Saving {{ .AppName }}-operator:local image as tarball..."
        until docker image inspect {{ .AppName }}-operator:local >/dev/null 2>&1; do
          echo "Waiting for {{ .AppName }}-operator:local image to be built..."
          sleep 2
        done
        docker save {{ .AppName }}-operator:local -o /images/operator.tar
{{- if .HasTargetAPI }}
        echo "Saving {{ .TargetAPIImage }} image as tarball..."
        docker pull {{ .TargetAPIImage }}
        docker save {{ .TargetAPIImage }} -o /images/target-api.tar
{{- end }}
{{- if .HasKubectlPlugin }}
        echo "Saving {{ .AppName }}-kubectl-plugin:latest image as tarball..."
        until docker image inspect {{ .AppName }}-kubectl-plugin:latest >/dev/null 2>&1; do
          echo "Waiting for {{ .AppName }}-kubectl-plugin:latest image to be built..."
          sleep 2
        done
        docker save {{ .AppName }}-kubectl-plugin:latest -o /images/kubectl-plugin.tar
{{- end }}
        echo "Downloading helm binary..."
        wget -q https://get.helm.sh/helm-v3.14.0-linux-amd64.tar.gz -O /tmp/helm.tar.gz
        tar -xzf /tmp/helm.tar.gz -C /tmp
        cp /tmp/linux-amd64/helm /images/helm
        chmod +x /images/helm
        echo "Images and tools saved successfully"
    depends_on:
      k3s-deploy-operator-build:
        condition: service_completed_successfully
{{- if .HasKubectlPlugin }}
      rundeck-docker-plugin-build:
        condition: service_completed_successfully
{{- end }}

  # Build the operator image
  k3s-deploy-operator-build:
    build:
      context: .
      dockerfile: Dockerfile
    image: {{ .AppName }}-operator:local
    profiles:
      - k3s-deploy
    entrypoint: ["/manager", "--version"]

  # Import images into k3s and deploy all workloads
  k3s-deploy-init:
    image: rancher/k3s:v1.29.0-k3s1
    profiles:
      - k3s-deploy
    network_mode: "service:k3s-deploy"
    volumes:
      - k3s-deploy-kubeconfig:/kubeconfig
      - k3s-deploy-images:/images:ro
      - k3s-deploy-run:/run/k3s:ro
      - ./chart/{{ .AppName }}:/chart/{{ .AppName }}:ro
{{- if .HasTargetAPI }}
      - ./config/target-api:/manifests:ro
{{- end }}
      - ./k3s-deploy-output:/host-output
    environment:
      - KUBECONFIG=/kubeconfig/kubeconfig-fixed.yaml
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "=== k3s-deploy-init: Setting up in-cluster deployment ==="

        echo "Waiting for kubeconfig..."
        until [ -f /kubeconfig/kubeconfig.yaml ]; do sleep 1; done

        echo "Fixing kubeconfig for container access..."
        cp /kubeconfig/kubeconfig.yaml /kubeconfig/kubeconfig-fixed.yaml
        chmod 644 /kubeconfig/kubeconfig-fixed.yaml

        echo "Copying kubeconfig for host access (port 6445)..."
        sed 's/6443/6445/g' /kubeconfig/kubeconfig.yaml > /host-output/kubeconfig.yaml
        chmod 644 /host-output/kubeconfig.yaml

        echo "Waiting for k3s API..."
        until kubectl get nodes; do sleep 2; done

        echo "--- Importing container images into k3s ---"
        until [ -f /images/operator.tar ]; do
          echo "Waiting for image tarballs..."
          sleep 2
        done
        echo "Waiting for containerd socket..."
        until [ -S /run/k3s/containerd/containerd.sock ]; do
          echo "  socket not ready yet..."
          sleep 2
        done
        echo "Importing operator image..."
        ctr --address /run/k3s/containerd/containerd.sock --namespace k8s.io images import /images/operator.tar
{{- if .HasTargetAPI }}
        echo "Importing target API image..."
        ctr --address /run/k3s/containerd/containerd.sock --namespace k8s.io images import /images/target-api.tar
{{- end }}
{{- if .HasKubectlPlugin }}
        echo "Importing kubectl plugin image..."
        ctr --address /run/k3s/containerd/containerd.sock --namespace k8s.io images import /images/kubectl-plugin.tar
{{- end }}
        echo "Images imported successfully"

        echo "--- Installing Helm ---"
        cp /images/helm /bin/helm
        helm version

        echo "--- Creating namespace ---"
        kubectl create namespace {{ .Namespace }}
{{- if .HasTargetAPI }}

        echo "--- Deploying target API ---"
        kubectl apply -f /manifests/deployment.yaml
{{- end }}

        echo "--- Installing operator via Helm chart ---"
        helm install {{ .AppName }} /chart/{{ .AppName }} \
          -n {{ .Namespace }} \
          --set controllerManager.manager.image.repository=docker.io/library/{{ .AppName }}-operator \
          --set controllerManager.manager.image.tag=local \
          --set controllerManager.manager.imagePullPolicy=Never \
          --set 'controllerManager.manager.args={--leader-elect,--namespace-scoped{{ if .HasTargetAPI }},--deployment-name={{ .AppName }},--base-path={{ .BasePath }}{{ end }}}'
{{- if .HasTargetAPI }}

        echo "--- Waiting for target API to be ready ---"
        kubectl rollout status deployment/{{ .AppName }} -n {{ .Namespace }} --timeout=120s
{{- end }}

        echo "--- Waiting for operator to be ready ---"
        kubectl rollout status deployment/{{ .AppName }}-controller-manager -n {{ .Namespace }} --timeout=120s

        echo "=== All workloads deployed successfully ==="
        echo "Access the cluster:"
        echo "  KUBECONFIG=./k3s-deploy-output/kubeconfig.yaml kubectl get pods -n {{ .Namespace }}"
    depends_on:
      k3s-deploy:
        condition: service_healthy
      k3s-deploy-image-save:
        condition: service_completed_successfully
{{- if .HasRundeckProject }}

  # ============================================================================
  # Rundeck: Run operator jobs via web UI (k3s-deploy profile)
  # ============================================================================

  # Rundeck server with API token for automated setup
  # Runs as root so job scripts can access the Docker socket for docker-based jobs
  rundeck:
    image: rundeck/rundeck:5.8.0
    user: root
    profiles:
      - k3s-deploy
    ports:
      - "4440:4440"
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        mkdir -p /tmp/remco-partials/framework
        echo "rundeck.tokens.file=/home/rundeck/etc/tokens.properties" > /tmp/remco-partials/framework/tokens.properties
        export PATH="/home/rundeck/server/data:$PATH"
        exec /home/rundeck/docker-lib/entry.sh
    environment:
      - PATH=/home/rundeck/server/data:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
      - RUNDECK_GRAILS_URL=http://localhost:4440
      - RUNDECK_DATABASE_DRIVER=org.h2.Driver
      - RUNDECK_DATABASE_URL=jdbc:h2:file:/home/rundeck/server/data/grailsdb;DB_CLOSE_ON_EXIT=FALSE;NON_KEYWORDS=MONTH,HOUR,MINUTE,YEAR
      # Docker-based job configuration (used by rundeck-docker-project jobs)
      - DOCKER_NETWORK={{ .AppName }}-operator-net
    volumes:
      - rundeck-data:/home/rundeck/server/data
      - ./rundeck-project/tokens.properties:/home/rundeck/etc/tokens.properties:ro
      - /var/run/docker.sock:/var/run/docker.sock
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:4440"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 60s
    depends_on:
      k3s-deploy-init:
        condition: service_completed_successfully

  # Build the kubectl plugin binary for linux/amd64
  rundeck-kubectl-build:
    image: golang:1.25
    profiles:
      - k3s-deploy
    volumes:
      - ./kubectl-plugin:/src:ro
      - rundeck-plugin-bin:/output
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Building kubectl plugin..."
        cp -r /src /build
        cd /build
        go mod tidy
        CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o /output/kubectl-{{ .AppName }} .
        chmod +x /output/kubectl-{{ .AppName }}
        echo "kubectl plugin built successfully"

  # Build kubectl plugin Docker image for docker-based Rundeck jobs
  rundeck-docker-plugin-build:
    build:
      context: ./kubectl-plugin
      dockerfile: Dockerfile
    image: {{ .AppName }}-kubectl-plugin:latest
    profiles:
      - k3s-deploy
    entrypoint: ["kubectl-{{ .AppName }}", "--help"]

  # Stage Docker CLI binary for Rundeck docker-based jobs
  rundeck-docker-cli-stage:
    image: docker:cli
    profiles:
      - k3s-deploy
    volumes:
      - rundeck-data:/rundeck-data
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        cp /usr/local/bin/docker /rundeck-data/docker
        chmod +x /rundeck-data/docker
        echo "Docker CLI staged to /rundeck-data/docker"

  # Configure Rundeck: install kubectl + plugin, create projects, import jobs
  rundeck-init:
    image: bitnami/kubectl:latest
    user: root
    profiles:
      - k3s-deploy
    volumes:
      - k3s-deploy-kubeconfig:/kubeconfig:ro
      - rundeck-plugin-bin:/plugin-bin:ro
      - ./rundeck-project:/rundeck-project:ro
      - ./rundeck-docker-project:/rundeck-docker-project:ro
      - ./rundeck-k8s-project:/rundeck-k8s-project:ro
      - rundeck-data:/rundeck-data
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "=== rundeck-init: Configuring Rundeck ==="

        RUNDECK_URL="http://rundeck:4440"
        TOKEN="letmein99"

        echo "--- Waiting for Rundeck API ---"
        until curl -sf -H "Accept: application/json" \
          -H "X-Rundeck-Auth-Token: $${TOKEN}" \
          "$${RUNDECK_URL}/api/14/system/info" 2>/dev/null | grep -q '"system"'; do
          echo "  Rundeck not ready yet..."
          sleep 5
        done
        echo "Rundeck API is ready"

        echo "--- Creating project: {{ .AppName }}-operator ---"
        curl -sf -X POST \
          -H "Accept: application/json" \
          -H "Content-Type: application/json" \
          -H "X-Rundeck-Auth-Token: $${TOKEN}" \
          -d '{"name":"{{ .AppName }}-operator","description":"Manage {{ .AppName }} resources via Kubernetes operator","config":{"project.name":"{{ .AppName }}-operator"}}' \
          "$${RUNDECK_URL}/api/14/projects" || echo "Project may already exist"

        echo "--- Importing job definitions (script-based) ---"
        for jobfile in /rundeck-project/jobs/resources/*.yaml \
                       /rundeck-project/jobs/queries/*.yaml \
                       /rundeck-project/jobs/actions/*.yaml \
                       /rundeck-project/jobs/operations/*.yaml; do
          if [ -f "$$jobfile" ]; then
            jobname=$$(basename "$$jobfile")
            echo "  Importing $${jobname}..."
            curl -sf -X POST \
              -H "Accept: application/json" \
              -H "Content-Type: application/yaml" \
              -H "X-Rundeck-Auth-Token: $${TOKEN}" \
              --data-binary @"$$jobfile" \
              "$${RUNDECK_URL}/api/14/project/{{ .AppName }}-operator/jobs/import?format=yaml&dupeOption=update" || echo "  Warning: failed to import $${jobname}"
          fi
        done

        echo "--- Creating project: {{ .AppName }}-operator-docker ---"
        curl -sf -X POST \
          -H "Accept: application/json" \
          -H "Content-Type: application/json" \
          -H "X-Rundeck-Auth-Token: $${TOKEN}" \
          -d '{"name":"{{ .AppName }}-operator-docker","description":"Manage {{ .AppName }} resources via Kubernetes operator (Docker execution)","config":{"project.name":"{{ .AppName }}-operator-docker"}}' \
          "$${RUNDECK_URL}/api/14/projects" || echo "Project may already exist"

        echo "--- Importing job definitions (Docker-based) ---"
        for jobfile in /rundeck-docker-project/jobs/resources/*.yaml \
                       /rundeck-docker-project/jobs/queries/*.yaml \
                       /rundeck-docker-project/jobs/actions/*.yaml \
                       /rundeck-docker-project/jobs/operations/*.yaml; do
          if [ -f "$$jobfile" ]; then
            jobname=$$(basename "$$jobfile")
            echo "  Importing $${jobname}..."
            curl -sf -X POST \
              -H "Accept: application/json" \
              -H "Content-Type: application/yaml" \
              -H "X-Rundeck-Auth-Token: $${TOKEN}" \
              --data-binary @"$$jobfile" \
              "$${RUNDECK_URL}/api/14/project/{{ .AppName }}-operator-docker/jobs/import?format=yaml&dupeOption=update" || echo "  Warning: failed to import $${jobname}"
          fi
        done

        echo "--- Creating project: {{ .AppName }}-operator-k8s ---"
        curl -sf -X POST \
          -H "Accept: application/json" \
          -H "Content-Type: application/json" \
          -H "X-Rundeck-Auth-Token: $${TOKEN}" \
          -d '{"name":"{{ .AppName }}-operator-k8s","description":"Manage {{ .AppName }} resources via Kubernetes operator (Kubernetes pod execution)","config":{"project.name":"{{ .AppName }}-operator-k8s"}}' \
          "$${RUNDECK_URL}/api/14/projects" || echo "Project may already exist"

        echo "--- Importing job definitions (Kubernetes pod-based) ---"
        for jobfile in /rundeck-k8s-project/jobs/resources/*.yaml \
                       /rundeck-k8s-project/jobs/queries/*.yaml \
                       /rundeck-k8s-project/jobs/actions/*.yaml \
                       /rundeck-k8s-project/jobs/operations/*.yaml; do
          if [ -f "$$jobfile" ]; then
            jobname=$$(basename "$$jobfile")
            echo "  Importing $${jobname}..."
            curl -sf -X POST \
              -H "Accept: application/json" \
              -H "Content-Type: application/yaml" \
              -H "X-Rundeck-Auth-Token: $${TOKEN}" \
              --data-binary @"$$jobfile" \
              "$${RUNDECK_URL}/api/14/project/{{ .AppName }}-operator-k8s/jobs/import?format=yaml&dupeOption=update" || echo "  Warning: failed to import $${jobname}"
          fi
        done

        echo "--- Installing kubectl and plugin into Rundeck ---"
        cp $$(which kubectl) /rundeck-data/kubectl
        chmod +x /rundeck-data/kubectl
        echo "kubectl binary staged"
        if [ -f /plugin-bin/kubectl-{{ .AppName }} ]; then
          cp /plugin-bin/kubectl-{{ .AppName }} /rundeck-data/kubectl-{{ .AppName }}
          chmod +x /rundeck-data/kubectl-{{ .AppName }}
          echo "Plugin binary staged"
        else
          echo "Warning: kubectl plugin binary not found"
        fi

        echo "--- Uploading Kubernetes credentials to Rundeck Key Storage ---"
        sed 's/127.0.0.1/k3s-deploy/g' /kubeconfig/kubeconfig.yaml > /tmp/kubeconfig.yaml
        export KUBECONFIG=/tmp/kubeconfig.yaml
        K8S_TOKEN=$$(kubectl create token {{ .AppName }}-plugin-runner -n {{ .Namespace }} --duration=8760h)
        K8S_URL="https://k3s-deploy:6443"
        for PROJECT in {{ .AppName }}-operator {{ .AppName }}-operator-docker {{ .AppName }}-operator-k8s; do
          echo "  Uploading credentials for project: $${PROJECT}"
          curl -sf -X POST \
            -H "X-Rundeck-Auth-Token: $${TOKEN}" \
            -H "Content-Type: application/x-rundeck-data-password" \
            --data-binary "$${K8S_TOKEN}" \
            "$${RUNDECK_URL}/api/14/storage/keys/project/$${PROJECT}/k8s-token" || echo "  Warning: failed to upload k8s-token for $${PROJECT}"
          curl -sf -X POST \
            -H "X-Rundeck-Auth-Token: $${TOKEN}" \
            -H "Content-Type: application/x-rundeck-data-password" \
            --data-binary "$${K8S_URL}" \
            "$${RUNDECK_URL}/api/14/storage/keys/project/$${PROJECT}/k8s-url" || echo "  Warning: failed to upload k8s-url for $${PROJECT}"
        done
        echo "Kubernetes credentials uploaded to Key Storage"

        echo "=== Rundeck setup complete ==="
        echo "Access Rundeck at: http://localhost:4440"
        echo "Default credentials: admin / admin"
        echo "Projects: {{ .AppName }}-operator (script), {{ .AppName }}-operator-docker (Docker), {{ .AppName }}-operator-k8s (Kubernetes)"
    depends_on:
      rundeck:
        condition: service_healthy
      rundeck-kubectl-build:
        condition: service_completed_successfully
      rundeck-docker-cli-stage:
        condition: service_completed_successfully
{{- end }}

volumes:
  k3s-data:
  k3s-kubeconfig:
  k3s-deploy-data:
  k3s-deploy-kubeconfig:
  k3s-deploy-images:
  k3s-deploy-run:
{{- if .HasRundeckProject }}
  rundeck-data:
    name: {{ .AppName }}-rundeck-data
  rundeck-plugin-bin:
{{- end }}

networks:
  default:
    name: {{ .AppName }}-operator-net
