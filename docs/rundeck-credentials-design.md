# Rundeck Kubernetes Credentials Design

## Current State

Rundeck authenticates to the Kubernetes cluster via a **kubeconfig file** mounted into the Rundeck container. The `rundeck-init` service extracts the kubeconfig from k3s and writes it to a shared volume:

```
/home/rundeck/server/data/kube/config
```

The `KUBECONFIG` environment variable on the Rundeck service points to this file. All three Rundeck project types depend on this kubeconfig, but in different ways:

| Project Type | How it uses credentials |
|---|---|
| **Script** (`rundeck-project`) | Rundeck server runs `kubectl petstore status` directly — kubectl reads `$KUBECONFIG` |
| **Docker** (`rundeck-docker-project`) | Rundeck runs `docker run -v $DOCKER_KUBE_VOLUME:/root/.kube:ro ...` — kubeconfig mounted into container |
| **K8s Pod** (`rundeck-k8s-project`) | Rundeck runs `kubectl run $POD ...` — kubectl reads `$KUBECONFIG` to create the ephemeral pod |

The **K8s pod-based jobs** have a two-layer auth model:
1. **Rundeck → K8s API**: needs credentials to run `kubectl run` (create the pod)
2. **Pod → K8s API**: uses in-cluster ServiceAccount (`plugin-runner`) — no Rundeck credentials needed

## Goal

Eliminate the kubeconfig file from Rundeck. Store Kubernetes credentials in **Rundeck Key Storage** and reference them from jobs via secure options. This aligns with how Rundeck's own Kubernetes plugins handle credentials (server URL + token from Key Storage).

## Authentication Method

### Why token-based (not kubeconfig)

k3s generates kubeconfigs with **client certificate auth**, not bearer tokens. Rundeck Key Storage stores passwords/tokens as simple strings — it can't natively store a multi-field kubeconfig with embedded certificates.

Options:
1. **Base64-encode the entire kubeconfig** and store as a password — scripts decode and write to temp file. Messy.
2. **Create a ServiceAccount token** via `kubectl create token` — clean, single string, standard K8s auth. This is how Rundeck's Kubernetes plugins work.

Token-based auth is the clear choice.

### Which ServiceAccount?

The generated `plugin-runner` ServiceAccount is bound to `manager-role` (generated by controller-gen). Its permissions:

```yaml
# manager-role permissions:
- pods: get, list, watch
- services: list, watch
- deployments, statefulsets: get, list, watch
- petstore.example.com CRDs: create, delete, get, list, patch, update, watch
- CRD finalizers: update
- CRD status: get, patch, update
```

**Sufficient for:**
- Script jobs — the kubectl plugin reads/writes CRDs (covered)
- K8s pod jobs (inside the pod) — same CRD operations (covered)

**Missing for:**
- K8s pod jobs (creating the pod) — needs `pods: create, delete` (only has get/list/watch)
- Docker jobs — needs pod create/delete if switched to token auth for `kubectl run`; the Docker execution itself doesn't go through K8s

### Option A: Widen `plugin-runner` permissions

Add pod create/delete to `manager-role`. Simple, but widens the operator's own permissions unnecessarily.

### Option B: Separate `plugin-runner-role` ClusterRole

Create a dedicated ClusterRole for `plugin-runner` that aggregates `manager-role` permissions plus pod create/delete:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: plugin-runner-role
rules:
# All manager-role permissions (for kubectl plugin CRD operations)
# Plus pod management (for kubectl run ephemeral pods)
- apiGroups: [""]
  resources: [pods]
  verbs: [create, delete, get, list, watch]
# Plus pod/attach and pod/log (for kubectl run -i streaming)
- apiGroups: [""]
  resources: [pods/attach, pods/log]
  verbs: [get, create]
```

This keeps the operator's `manager-role` minimal while giving the plugin runner what it needs. The ClusterRoleBinding for `plugin-runner` would reference `plugin-runner-role` instead of `manager-role`.

**Recommendation: Option B** — separate role with explicit pod management permissions.

## Key Storage Design

### Stored secrets

| Key Storage Path | Type | Content |
|---|---|---|
| `keys/project/{app}-operator/k8s-token` | password | SA bearer token from `kubectl create token` |
| `keys/project/{app}-operator/k8s-url` | password | K8s API server URL (e.g., `https://k3s-deploy:6443`) |

CA cert is omitted — use `--insecure-skip-tls-verify` for simplicity in the dev/demo Docker Compose environment. Production deployments would add a CA cert to Key Storage.

### Job secure options

Every job across all three projects would include hidden secure options:

```yaml
options:
- name: k8s_token
  secure: true
  valueExposed: true
  storagePath: keys/project/{app}-operator/k8s-token
  hidden: true
- name: k8s_url
  secure: true
  valueExposed: true
  storagePath: keys/project/{app}-operator/k8s-url
  hidden: true
```

`valueExposed: true` is required — without it, secure option values are not available to scripts. `hidden: true` keeps them out of the job run form. `storagePath` auto-populates from Key Storage.

## Per-Project Script Changes

### Script jobs

Straightforward — add `--server`/`--token`/`--insecure-skip-tls-verify` flags to kubectl:

```bash
# Before:
kubectl petstore status -n petstore-system

# After:
kubectl --server="$RD_OPTION_K8S_URL" \
        --token="$RD_OPTION_K8S_TOKEN" \
        --insecure-skip-tls-verify \
        petstore status -n petstore-system
```

### K8s pod jobs

Same pattern — the `kubectl run` command needs the flags:

```bash
# Before:
kubectl run $POD --image=$IMAGE ...

# After:
kubectl --server="$RD_OPTION_K8S_URL" \
        --token="$RD_OPTION_K8S_TOKEN" \
        --insecure-skip-tls-verify \
        run $POD --image=$IMAGE ...
```

### Docker jobs

The kubectl plugin is built with cobra/client-go, which means it inherits the standard `--server`, `--token`, and `--insecure-skip-tls-verify` flags automatically. No code changes to the plugin are needed — just pass the flags through to the container:

```bash
# Before (kubeconfig volume mount):
docker run --rm -i \
  --network=$DOCKER_NETWORK \
  -v $DOCKER_KUBE_VOLUME:/root/.kube:ro \
  petstore-kubectl-plugin status -n petstore-system

# After (token flags, no volume mount):
docker run --rm -i \
  --network=$DOCKER_NETWORK \
  petstore-kubectl-plugin \
  status -n petstore-system \
  --server="$RD_OPTION_K8S_URL" \
  --token="$RD_OPTION_K8S_TOKEN" \
  --insecure-skip-tls-verify
```

This makes all three project types uniform — same three flags appended to the command. No temp files, no kubeconfig construction, no shell wrappers. The Docker job execution model stays clean.

## Docker Compose Changes

### `rundeck-init` service

Replace kubeconfig file write with Key Storage API uploads:

```bash
# Remove this:
mkdir -p /rundeck-data/kube
sed 's/127.0.0.1/k3s-deploy/g' /kubeconfig/kubeconfig.yaml > /rundeck-data/kube/config

# Add this:
echo "--- Uploading Kubernetes credentials to Rundeck Key Storage ---"
K8S_TOKEN=$(kubectl create token plugin-runner -n {namespace} --duration=8760h)

# Upload token (shared across all projects)
for PROJECT in {app}-operator {app}-operator-docker {app}-operator-k8s; do
  curl -sf -X POST \
    -H "X-Rundeck-Auth-Token: ${TOKEN}" \
    -H "Content-Type: application/x-rundeck-data-password" \
    --data-binary "${K8S_TOKEN}" \
    "${RUNDECK_URL}/api/14/storage/keys/project/${PROJECT}/k8s-token"

  curl -sf -X POST \
    -H "X-Rundeck-Auth-Token: ${TOKEN}" \
    -H "Content-Type: application/x-rundeck-data-password" \
    --data-binary "https://k3s-deploy:6443" \
    "${RUNDECK_URL}/api/14/storage/keys/project/${PROJECT}/k8s-url"
done
```

### `rundeck` service

- Remove `KUBECONFIG` environment variable
- Remove kubeconfig volume mount (or keep it only for kubectl binary access)
- The `DOCKER_KUBE_VOLUME` env var is no longer needed

## Template Changes Summary

### New files
- `pkg/templates/plugin_runner_role.yaml.tmpl` — dedicated ClusterRole with pod create/delete

### Modified templates (all 27 job templates)

**Script project** (9 templates):
- Add `k8s_token` and `k8s_url` secure options
- Prepend `--server`/`--token`/`--insecure-skip-tls-verify` to all kubectl commands

**Docker project** (9 templates):
- Add `k8s_token` and `k8s_url` secure options
- Remove kubeconfig volume mount (`-v $DOCKER_KUBE_VOLUME:/root/.kube:ro`)
- Append `--server`/`--token`/`--insecure-skip-tls-verify` flags to plugin commands
- Remove `DOCKER_KUBE_VOLUME` references

**K8s pod project** (9 templates):
- Add `k8s_token` and `k8s_url` secure options
- Prepend `--server`/`--token`/`--insecure-skip-tls-verify` to `kubectl run` commands

### Modified generator files
- `pkg/templates/docker_compose.yaml.tmpl` — update `rundeck-init` to use Key Storage API
- `pkg/templates/plugin_role_binding.yaml.tmpl` — reference `plugin-runner-role` instead of `manager-role`
- `pkg/generator/controller.go` — generate `plugin_runner_role.yaml`
- `pkg/templates/kustomization_rbac.yaml.tmpl` — add `plugin_runner_role.yaml`
- `pkg/templates/templates.go` — add embed for new role template

## Decisions

1. **Token expiration**: 1-year token (`--duration=8760h`) is acceptable.

2. **TLS verification**: Use `--insecure-skip-tls-verify` for now. CA cert in Key Storage for proper TLS verification will be added in a future iteration.

3. **Key Storage scope**: Per-project (`keys/project/{app}-operator/k8s-token`). Each project gets its own copy of the credentials.

4. **Backward compatibility**: No fallback. The kubeconfig-based approach will be removed entirely.
